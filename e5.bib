Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Holler,
author = {Holler, Judith and Tutton, Mark and Wilkin, Katie and Planck, Max},
file = {:home/josiah/Documents/Mendeley Desktop/Holler et al. - Unknown - Co - speech gestures in the process of meaning coordination School of Psychological Sciences , Oxford Road , U.pdf:pdf},
keywords = {co-speech gesture,common ground,referential communication,repeated reference,visibility},
title = {{Co - speech gestures in the process of meaning coordination School of Psychological Sciences , Oxford Road , University of Manchester}}
}
@article{Masson-Carro2015,
abstract = {ABSTRACTHand gestures are tightly coupled with speech and with action. Hence, recent accounts have emphasised the idea that simulations of spatio-motoric imagery underlie the production of co-speech gestures. In this study, we suggest that action simulations directly influence the iconic strategies used by speakers to translate aspects of their mental representations into gesture. Using a classic referential paradigm, we investigate how speakers respond gesturally to the affordances of objects, by comparing the effects of describing objects that afford action performance (such as tools) and those that do not, on gesture production. Our results suggest that affordances play a key role in determining the amount of representational (but not non-representational) gestures produced by speakers, and the techniques chosen to depict such objects. To our knowledge, this is the first study to systematically show a connection between object characteristics and representation techniques in spontaneous gesture product...},
author = {Masson-Carro, Ingrid and Goudbeek, Martijn and Krahmer, Emiel},
doi = {10.1080/23273798.2015.1108448},
file = {:home/josiah/Documents/Mendeley Desktop/Masson-Carro, Goudbeek, Krahmer - 2015 - Can you handle this The impact of object affordances on how co-speech gestures are produced.pdf:pdf},
issn = {2327-3798},
journal = {Language, Cognition and Neuroscience},
keywords = {Gesture,action,affordances,representation techniques,simulation},
number = {March},
pages = {1--11},
pmid = {27226970},
title = {{Can you handle this? The impact of object affordances on how co-speech gestures are produced}},
url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1108448},
volume = {3798},
year = {2015}
}
@article{Goldin-Meadow2001,
abstract = {Why is it that people cannot keep their hands still when they talk? One reason may be that gesturing actually lightens cognitive load while a person is thinking of what to say. We asked adults and children to remember a list of letters or words while explaining how they solved a math problem. Both groups remembered significantly more items when they gestured during their math explanations than when they did not gesture. Gesturing appeared to save the speakers' cognitive resources on the explanation task, permitting the speakers to allocate more resources to the memory task. It is widely accepted that gesturing reflects a speaker's cognitive state, but our observations suggest that, by reducing cognitive load, gesturing may also play a role in shaping that state.},
author = {Goldin-Meadow, Susan and Nusbaum, H and Kelly, Spencer D. and Wagner, S},
doi = {10.1111/1467-9280.00395},
file = {:home/josiah/Documents/Mendeley Desktop/Goldin-Meadow et al. - 2001 - Explaining math gesturing lightens the load.pdf:pdf},
isbn = {0956-7976 (Print)$\backslash$r0956-7976 (Linking)},
issn = {0956-7976},
journal = {Psychological science : a journal of the American Psychological Society / APS},
number = {6},
pages = {516--522},
pmid = {11760141},
title = {{Explaining math: gesturing lightens the load.}},
volume = {12},
year = {2001}
}
@article{Melinger2004,
abstract = {[This paper aims to determine whether iconic tracing gestures produced while speaking constitute part of the speaker's communicative intention. We used a picture description task in which speakers must communicate the spatial and color information of each picture to an interlocutor. By establishing the necessary minimal content of an intended message, we determined whether speech produced with concurrent gestures is less explicit than speech without gestures. We argue that a gesture must be communicatively intended if it expresses necessary information that was nevertheless omitted from speech. We found that speakers who produced iconic gestures representing spatial relations omitted more required spatial information from their descriptions than speakers who did not gesture. These results provide evidence that speakers intend these gestures to communicate. The results have implications for the cognitive architectures that underlie the production of gesture and speech.]},
author = {Melinger, Alissa and Levelt, Willem J M},
doi = {10.1075/gest.4.2.02mel},
file = {:home/josiah/Documents/Mendeley Desktop/Melinger, Levelt - 2004 - Gesture and the communicative intention of the speaker.pdf:pdf},
issn = {15681475},
journal = {Gesture},
keywords = {communica,gesture,interaction,speech production},
number = {2004},
pages = {119--141},
title = {{Gesture and the communicative intention of the speaker}},
volume = {4(2)},
year = {2004}
}
@article{Holler2007,
abstract = {Past research has investigated the impact of mutual knowledge on communication by focusing mainly on verbal communication. This study uses a wider focus, which includes speech and gesture. Speakers completed a referential communication task with recipients who did or did not share with them knowledge about the size of certain entities. The results showed that when such common ground exists between interlocutors, speakers' use of gesture and speech is affected. The main finding was that when speakers talked to recipients for whom the size information was new information, they represented this information predominantly in gesture only or in gesture and speech. However, when speakers talked to recipients with whom they shared knowledge about the entities' size, speakers encoded this information mainly verbally but not gesturally. The results are interpreted with respect to past research into common ground and language use, the pragmatics of gesture, and theories of gesture production. [ABSTRACT FROM AUTHOR]},
author = {Holler, Judith and Stevens, Rachel},
doi = {10.1177/0261927X06296428},
file = {:home/josiah/Documents/Mendeley Desktop/Holler, Stevens - 2007 - The effect of common ground on how speakers use gesture and speech to represent size information.pdf:pdf},
isbn = {0261-927X},
issn = {0261-927X},
journal = {Journal of Language and Social Psychology},
keywords = {CUED speech,GENERAL semantics,GESTURE,LANGUAGE {\&} languages,LECTURERS,NONVERBAL communication,ORAL communication,SIGN language,SYMBOLIC communication,common ground,iconic hand gestures,recipient design,size information},
number = {1},
pages = {4--27},
title = {{The effect of common ground on how speakers use gesture and speech to represent size information}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=ufh{\&}AN=24195444{\&}site=ehost-live{\%}5Cnhttp://jls.sagepub.com/cgi/doi/10.1177/0261927X06296428},
volume = {26},
year = {2007}
}
@incollection{Kita2000,
author = {Kita, Sotaro},
booktitle = {Language and gesture},
editor = {McNeill, David},
pages = {162--185},
publisher = {Cambridge University Press},
title = {{How representational gestures help speaking}},
volume = {1},
year = {2000}
}
@article{Holler2011,
abstract = {[Mimicry has been observed regarding a range of nonverbal behaviors, but only recently have researchers started to investigate mimicry in co-speech gestures. These gestures are considered to be crucially different from other aspects of nonverbal behavior due to their tight link with speech. This study provides evidence of mimicry in co-speech gestures in face-to-face dialogue, the most common forum of everyday talk. In addition, it offers an analysis of the functions that mimicked co-speech gestures fulfill in the collaborative process of creating a mutually shared understanding of referring expressions. The implications bear on theories of gesture production, research on grounding, and the mechanisms underlying behavioral mimicry.]},
author = {Holler, Judith and Wilkin, Katie},
doi = {10.1007/s10919-011-0105-6},
file = {:home/josiah/Documents/Mendeley Desktop/Holler, Wilkin - 2011 - Co-Speech Gesture Mimicry in the Process of Collaborative Referring During Face-to-Face Dialogue.pdf:pdf},
isbn = {1091901101056},
issn = {01915886},
journal = {Journal of Nonverbal Behavior},
keywords = {Co-speech gestures,Collaborative referring,Common ground,Grounding,Mimicry},
number = {2},
pages = {133--153},
title = {{Co-Speech Gesture Mimicry in the Process of Collaborative Referring During Face-to-Face Dialogue}},
volume = {35},
year = {2011}
}
@book{VanderSluis2007,
abstract = {This article presents a new computational model for the generation of multimodal referring expressions (REs), based on observations in human communication. The algorithm is an extension of the graph-based algorithm proposed by Krahmer, van Erk, and Verleg (2003) and makes use of a so-called Flashlight Model for pointing. The Flashlight Model accounts for various types of pointing gestures of different precisions. Based on a notion of effort, the algorithm produces REs combining language and pointing gestures. The algorithm is evaluated using two production experiments with which spontaneous data is gathered on controlled input. The output of the algorithm coincides to a large extent with the utterances of the participants. However, an important difference is that the participants tend to produce overspecified REs, whereas the algorithm generates minimal ones. This article briefly discusses ways to generate overspecified multimodal references.},
author = {van der Sluis, Ielka and Krahmer, Emiel},
booktitle = {Discourse Processes},
doi = {10.1080/01638530701600755},
file = {:home/josiah/Documents/Mendeley Desktop/van der Sluis, Krahmer - 2007 - Generating Multimodal References.pdf:pdf},
isbn = {0163853070160},
issn = {0163-853X},
number = {April 2014},
pages = {145--174},
title = {{Generating Multimodal References}},
volume = {44},
year = {2007}
}
@article{Morsella2004,
abstract = {Co-speech gestures traditionally have been considered communicative, but they may also serve other functions. For example, hand-arm movements seem to facilitate both spatial working memory and speech production. It has been proposed that gestures facilitate speech indirectly by sustaining spatial representations in working memory. Alternatively, gestures may affect speech production directly by activating embodied semantic representations involved in lexical search. Consistent with the first hypothesis, we found participants gestured more when describing visual objects from memory and when describing objects that were difficult to remember and encode verbally. However, they also gestured when describing a visually accessible object, and gesture restriction produced dysfluent speech even when spatial memory was untaxed, suggesting that gestures can directly affect both spatial memory and lexical retrieval.},
author = {Morsella, Ezequiel and Krauss, Robert M},
doi = {10.2307/4149008},
file = {:home/josiah/Documents/Mendeley Desktop/Morsella, Krauss - 2004 - The Role of Gestures in Spatial Working Memrory and Speech(2).pdf:pdf},
isbn = {0002-9556 (Print)},
issn = {00029556},
journal = {American Journal of Psychology},
keywords = {Adult,Female,Gestures,Humans,Male,Memory,Space Perception,Speech},
number = {3},
pages = {411--424},
pmid = {15457809},
title = {{The Role of Gestures in Spatial Working Memrory and Speech}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15457809},
volume = {117},
year = {2004}
}
@article{Holler2003,
abstract = {Two studies are reported that investigate how speakers use gesture in association with verbal ambiguity in two communicational situations characteristic of everyday talk. The first study uses a design that mimics a speaker's self-repair initiated by the listener, while the second study involves speakers producing longer stretches of speech involving lexical ambiguity, without the listener interacting verbally with the speaker. The findings of both studies show that speakers do use gesture to clarify verbal ambiguity. Moreover, they suggest that the speaker's awareness of a potential communication problem, and the fact that this communication problem is associated with the speech itself, are crucial variables influencing speakers' gestural behaviour. Differences in the complexity of the form of the gestures are also observed and the theoretical implications of this are discussed. Overall, these studies provide important insights into semantic and pragmatic aspects of representational hand gestures and speech-gesture interaction in everyday talk. [ABSTRACT FROM AUTHOR]},
author = {Holler, Judith and Beattie, Geoffrey W.},
doi = {10.1075/gest.3.2.02hol},
file = {:home/josiah/Documents/Mendeley Desktop/Holler, Beattie - 2003 - Pragmatic aspects of representational gestures Do speakers use them to clarify verbal ambiguity for the listene.PDF:PDF},
issn = {15681475},
journal = {Gesture},
keywords = {gestures,gestures assisting communication,iconic hand gestures,pragmatic aspects of representational,verbal ambiguity},
number = {2},
pages = {127--154},
title = {{Pragmatic aspects of representational gestures: Do speakers use them to clarify verbal ambiguity for the listener?}},
volume = {3},
year = {2003}
}
@article{Mol2012,
abstract = {Interlocutors sometimes repeat each other's co-speech hand gestures. In three experiments, we investigate to what extent the copying of such gestures' form is tied to their meaning in the linguistic context, as well as to interlocutors' representations of this meaning at the conceptual level. We found that gestures were repeated only if they could be interpreted within the meaningful context provided by speech. We also found evidence that the copying of gesture forms is mediated by representations of meaning. That is, representations of meaning are also converging across interlocutors rather than just representations of gesture form. We conclude that the repetition across interlocutors of representational hand gestures may be driven by representations at the conceptual level, as has also been proposed for the repetition of referring expressions across interlocutors (lexical entrainment). That is, adaptation in gesture resembles adaptation in speech, rather than it being an instance of automated motor-mimicry. {\textcopyright} 2011 Elsevier Inc.},
author = {Mol, Lisette and Krahmer, Emiel and Maes, Alfons and Swerts, Marc},
doi = {10.1016/j.jml.2011.07.004},
file = {:home/josiah/Documents/Mendeley Desktop/Mol et al. - 2012 - Adaptation in gesture Converging hands or converging minds.pdf:pdf},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {Adaptation,Alignment,Gesture,Lexical entrainment,Mimicry},
number = {1},
pages = {249--264},
publisher = {Elsevier Inc.},
title = {{Adaptation in gesture: Converging hands or converging minds?}},
url = {http://dx.doi.org/10.1016/j.jml.2011.07.004},
volume = {66},
year = {2012}
}
@article{Guellai2014,
abstract = {In everyday life, speech is accompanied by gestures. In the present study, two experiments tested the possibility that spontaneous gestures accompanying speech carry prosodic information. Experiment 1 showed that gestures provide prosodic information, as adults are able to perceive the congruency between low-pass filtered—thus unintelligible—speech and the gestures of the speaker. Experiment 2 shows that in the case of ambiguous sentences (i.e., sentences with two alternative meanings depending on their prosody) mismatched prosody and gestures lead participants to choose more often the meaning signaled by gestures. Our results demonstrate that the prosody that characterizes speech is not a modality specific phenomenon: it is also perceived in the spontaneous gestures that accompany speech.We drawthe conclusion that spontaneous gestures and speech form a single communication system where the suprasegmental aspects of spoken language are mapped to the motor-programs responsible for the production of both speech sounds and hand gestures.},
author = {Guella{\"{i}}, Bahia and Langus, Alan and Nespor, Marina},
doi = {10.3389/fpsyg.2014.00700},
file = {:home/josiah/Documents/Mendeley Desktop/Guella{\"{i}}, Langus, Nespor - 2014 - Prosody in the hands of the speaker.pdf:pdf},
isbn = {1664-1078 (Electronic)
1664-1078 (Linking)},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Ambiguity,Comprehension,Gestures,Prosody,Speech perception},
number = {JUL},
pages = {1--8},
pmid = {25071666},
title = {{Prosody in the hands of the speaker}},
volume = {5},
year = {2014}
}
@article{Wesp2001,
abstract = {Recent theories suggest alternatives to the commonly held belief that the sole role of gestures is to communicate meaning directly to listeners. Evidence suggests that gestures may serve a cognitive function for speakers, possibly acting as lexical primes. We observed that participants gestured more often when describing a picture from memory than when the picture was present and that gestures were not influenced by manipulating eye contact of a listener. We argue that spatial imagery serves a short-term memory function during lexical search and that gestures may help maintain spatial images. When spatial imagery is not necessary, as in conditions of direct visual stimulation, reliance on gestures is reduced or eliminated.},
author = {Wesp, Richard and Hesse, Jennifer and Keutmann, Donna and Wheaton, Karen},
doi = {10.2307/1423612},
file = {:home/josiah/Documents/Mendeley Desktop/Wesp et al. - 2001 - Gestures maintain spatial imagery.pdf:pdf},
isbn = {0002-9556 (Print)$\backslash$r0002-9556 (Linking)},
issn = {00029556},
journal = {American Journal of Psychology},
number = {4},
pages = {591--600},
pmid = {11789342},
title = {{Gestures maintain spatial imagery}},
volume = {114},
year = {2001}
}
@article{Scharp2010,
author = {Scharp, Victoria L and Tompkins, Connie A and Iverson, Jana M},
doi = {10.1080/02687030701192273.Gesture},
file = {:home/josiah/Documents/Mendeley Desktop/Scharp, Tompkins, Iverson - 2010 - NIH Public Access.pdf:pdf},
pages = {717--725},
title = {{NIH Public Access}},
volume = {21},
year = {2010}
}
@article{Rose2001,
abstract = {This study investigated the differential facilitation effects of gesture and visualisation processes on object naming in individuals with aphasia. Six participants with word production deficits resulting from varying levels of impairment in the word production system, underwent a series of naming trials. Baseline measures of naming were compared to those obtained following instructions to point, visualise, and produce gesture. The results supported the superiority of iconic gesture as a facilitator of object naming in aphasia. In particular, individuals with phonological access, storage, or encoding difficulties demonstrated significantly enhanced naming abilities with the use of iconic gesture as compared to individuals with a semantic impairment or an apraxia of speech. Pointing, cued articulation, and visualisation processes did not significantly enhance naming skills in these individuals. These results are discussed within the model of lexical gesture and word production proposed by Krauss and Hadar (1999).},
author = {Rose, Miranda and Douglas, Jacinta},
doi = {10.1080/02687040143000339},
file = {:home/josiah/Documents/Mendeley Desktop/Rose, Douglas - 2001 - The differential facilitatory effects of gesture and visualisation processes on object naming in aphasia.pdf:pdf},
isbn = {0268-7038},
issn = {0268-7038},
journal = {Aphasiology},
number = {10},
pages = {977--990},
title = {{The differential facilitatory effects of gesture and visualisation processes on object naming in aphasia}},
url = {http://www.tandfonline.com/doi/abs/10.1080/02687040143000339{\%}5Cnhttp://www.informaworld.com/10.1080/02687040143000339},
volume = {15},
year = {2001}
}
@article{Hoetjes2015,
abstract = {In dialogue, repeated references contain fewer words (which are also acoustically reduced) and fewer gestures than initial ones. In this paper, we describe three experiments studying to what extent gesture reduction is comparable to other forms of linguistic reduction. Since previous studies showed conflicting findings for gesture rate, we systematically compare two measures of gesture rate: gesture rate per word and per semantic attribute (Experiment I). In addition, we ask whether repetition impacts the form of gestures, by manual annotation of a number of features (Experiment I), by studying gradient differences using a judgment test (Experiment II), and by investigating how effective initial and repeated gestures are at communicating information (Experiment III). The results revealed no reduction in terms of gesture rate per word, but a U-shaped reduction pattern for gesture rate per attribute. Gesture annotation showed no reliable effects of repetition on gesture form, yet participants judged gestures from repeated references as less precise than those from initial ones. Despite this gradient reduction, gestures from initial and repeated references were equally successful in communicating information. Besides effects of repetition, we found systematic effects of visibility on gesture production, with more, longer, larger and more communicative gestures when participants could see each other. We discuss the implications of our findings for gesture research and for models of speech and gesture production.},
author = {Hoetjes, Marieke and Koolen, Ruud and Goudbeek, Martijn and Krahmer, Emiel and Swerts, Marc},
doi = {10.1016/j.jml.2014.10.004},
file = {:home/josiah/Documents/Mendeley Desktop/Hoetjes et al. - 2015 - Reduction in gesture during the production of repeated references.pdf:pdf},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {Gesture,Reduction,Repeated references,Visibility},
pages = {1--17},
publisher = {Elsevier Inc.},
title = {{Reduction in gesture during the production of repeated references}},
url = {http://dx.doi.org/10.1016/j.jml.2014.10.004},
volume = {79-80},
year = {2015}
}
@book{Clark1996,
author = {Clark, Herbert H},
publisher = {Cambridge university press},
title = {{Using language}},
year = {1996}
}
@article{Kimbara2008,
abstract = {Pairs of speakers participated in a joint-description task involving 10 cartoon clips. In a within-participants design that varied whether speakers could see each other, dyads described the content },
author = {Kimbara, Irene},
doi = {10.1007/s10919-007-0044-4},
file = {:home/josiah/Documents/Mendeley Desktop/Kimbara - 2008 - Gesture form convergence in joint description.pdf:pdf},
isbn = {0191-5886$\backslash$r1573-3653},
issn = {01915886},
journal = {Journal of Nonverbal Behavior},
keywords = {Form convergence,Gesture,Handshape,Joint description},
number = {2},
pages = {123--131},
title = {{Gesture form convergence in joint description}},
volume = {32},
year = {2008}
}
@article{Hostetter2007,
abstract = {The Information Packaging Hypothesis (Kita, 2000) holds that gestures play a role in conceptualising information for speaking. According to this view, speakers will gesture more when describing difficult-to-conceptualise information than when describing easy-to-conceptualise information. In the present study, 24 participants described ambiguous dot patterns under two conditions. In the dots-plus-shapes condition, geometric shapes connected the dots, and participants described the patterns in terms of those shapes. In the dots-only condition, no shapes were present, and participants generated their own geometric conceptualisations and described the patterns. Participants gestured at a higher rate in the dots-only condition than in the dots-plus-shapes condition. The results support the Information Packaging Hypothesis and suggest that gestures occur when information is difficult to conceptualise. [ABSTRACT FROM AUTHOR]},
author = {Hostetter, Autumn B. and Alibali, Martha W. and Kita, Sotaro},
doi = {10.1080/01690960600632812},
file = {:home/josiah/Documents/Mendeley Desktop/Hostetter, Alibali, Kita - 2007 - I see it in my hands' eye Representational gestures reflect conceptual demands.pdf:pdf},
issn = {0169-0965},
journal = {Language and Cognitive Processes},
number = {3},
pages = {313--336},
title = {{I see it in my hands' eye: Representational gestures reflect conceptual demands}},
volume = {22},
year = {2007}
}
@article{Driskell2003,
abstract = {Hand gestures are ubiquitous in communication. However, there is considerable debate regarding the fundamental role that gesture plays in communication and, subsequently, regarding the value of gesture for telecommunications. Controversy exists regarding whether gesture has a primarily communicative function (enhancing listener comprehension) or a primarily noncommunicative function (enhancing speech production). Moreover, some have argued that gesture seems to enhance listener comprehension only because of the effect gesture has on speech production. The purpose of this study was to examine the extent to which gesture enhances listener comprehension and the extent to which the effect of gesture on listener comprehension is mediated by the effects of gesture on speech production. Results indicated that gesture enhanced both listener comprehension and speech production. When the effects of gesture on speech production were controlled, the relationship between gesture and listener comprehension was reduced but still remained significant. These results suggest that gesture aids the listener as well as the speaker and that gesture has a direct effect on listener comprehension, independent of the effects gesture has on speech production. Implications for understanding the value of gestural information in telecommunications are discussed. Potential applications of this research include the design of computer-mediated communication systems and displays in which the visibility of gestures may be beneficial.},
author = {Driskell, James E and Radtke, Paul H},
doi = {10.1518/hfes.45.3.445.27258},
file = {:home/josiah/Documents/Mendeley Desktop/Driskell, Radtke - 2003 - The effect of gesture on speech production and comprehension(2).pdf:pdf},
isbn = {0018-7208},
issn = {0018-7208},
journal = {Human factors},
keywords = {Communication,Comprehension,Comprehension: physiology,Female,Gestures,Humans,Male,Military Personnel,Nonverbal Communication,Nonverbal Communication: psychology,Speech,Speech Production Measurement,Verbal Behavior},
number = {3},
pages = {445--54},
pmid = {14702995},
title = {{The effect of gesture on speech production and comprehension.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14702995},
volume = {45},
year = {2003}
}
@misc{Krauss1995,
abstract = {Three experiments investigated the extent to which spontaneous gestural accompaniments to a spoken message enhance the message′s communicative effectiveness. All three employed a modified referential communication task in which subjects (speakers) were videotaped as they described a stimulus to a partner, who then tried to select it from a set of similar stimuli. Half of the dyads communicated face-to-face; the remainder were in different rooms and communicated over an intercom. The videotaped descriptions were presented to the new subjects (listeners), who tried to select the stimulus described. Half of these listeners both saw and heard the videotape; the remainder only heard the soundtrack. The three experiments differed in the type of stimulus the speakers described: abstract graphic designs, novel synthesized sounds, or samples of tea. Communication accuracy (i.e., the rate at which listeners selected the correct stimulus) was better than chance in all three experiments, but in none was accuracy enhanced by allowing the listener to see the speaker′s gestures. The results bring into question the assumption that the communication of semantic information is a primary function of conversational hand gestures.},
author = {Krauss, Robert M. and Dushay, Robert a. and Chen, Yihsiu and Rauscher, Frances},
booktitle = {Journal of Experimental Social Psychology},
doi = {10.1006/jesp.1995.1024},
file = {:home/josiah/Documents/Mendeley Desktop/Krauss et al. - 1995 - The Communicative Value of Conversational Hand Gesture.pdf:pdf},
isbn = {0022-1031},
issn = {00221031},
number = {6},
pages = {533--552},
title = {{The Communicative Value of Conversational Hand Gesture}},
url = {http://www.sciencedirect.com/science/article/pii/S0022103185710244},
volume = {31},
year = {1995}
}
@book{McNeill1992,
author = {McNeill, David},
publisher = {University of Chicago press},
title = {{Hand and mind: What gestures reveal about thought}},
year = {1992}
}
@article{DeRuiter2006,
abstract = {As Rose (2006) discusses in the lead article, two camps can be identified in the field of gesture research: those who believe that gesticulation enhances communication by providing extra information to the listener, and on the other hand those who believe that gesticulation is not communicative, but rather that it facilitates speaker-internal word finding processes. I review a number of key studies relevant for this controversy, and conclude that the available empirical evidence is supporting the notion that gesture is a communicative device which can compensate for problems in speech by providing information in gesture. Following that, I discuss the finding by Rose and Douglas (2001) that making gestures does facilitate word production in some patients with aphasia. I argue that the gestures produced in the experiment by Rose and Douglas are not guaranteed to be of the same kind as the gestures that are produced spontaneously under naturalistic, communicative conditions, which makes it difficult to generalise from that particular study to general gesture behavior. As a final point, I encourage researchers in the area of aphasia to put more emphasis on communication in naturalistic contexts (e.g., conversation) in testing the capabilities of people with aphasia.},
author = {de Ruiter, Jan Peter},
doi = {10.1080/14417040600667285},
file = {:home/josiah/Documents/Mendeley Desktop/de Ruiter - 2006 - Can gesticulation help aphasic people speak, or rather, communicate.pdf:pdf},
isbn = {1441-7049 {\%}[ May 25, 2007},
issn = {14417049},
journal = {Advances in Speech Language Pathology},
keywords = {Aphasia,Gesticulation,Gesture,Lexical retrieval},
number = {2},
pages = {124--127},
title = {{Can gesticulation help aphasic people speak, or rather, communicate?}},
volume = {8},
year = {2006}
}
@article{Hostetter2011,
abstract = {Do the gestures that speakers produce while talking significantly benefit listeners' comprehension of the message? This question has been the topic of many research studies over the previous 35 years, and there has been little consensus. The present meta-analysis examined the effect sizes from 63 samples in which listeners' understanding of a message was compared when speech was presented alone with when speech was presented with gestures. It was found that across samples, gestures do provide a significant, moderate benefit to communication. Furthermore, the magnitude of this effect is moderated by 3 factors. First, effects of gesture differ as a function of gesture topic, such that gestures that depict motor actions are more communicative than those that depict abstract topics. Second, effects of gesture on communication are larger when the gestures are not completely redundant with the accompanying speech; effects are smaller when there is more overlap between the information conveyed in the 2 modalities. Third, the size of the effect of gesture is dependent on the age of the listeners, such that children benefit more from gestures than do adults. Remaining questions for future research are highlighted.},
author = {Hostetter, Autumn B},
doi = {10.1037/a0022128},
file = {:home/josiah/Documents/Mendeley Desktop/Hostetter - 2011 - When do gestures communicate A meta-analysis(3).pdf:pdf},
isbn = {0033-2909},
issn = {0033-2909},
journal = {Psychological bulletin},
keywords = {communication,comprehension,gesture},
number = {2},
pages = {297--315},
pmid = {21355631},
title = {{When do gestures communicate? A meta-analysis.}},
volume = {137},
year = {2011}
}
@article{Bangerter2004,
abstract = {Pointing was shown to focus attention in dialogue. Pairs of people talked and gestured to identify targets from arrays visible to both of them. Arrays were located at five distances: arm length (0 cm), 25 cm, 50 cm, 75 cm, and 100 cm. Some pairs could point; others could not. People relied more on pointing and less on language as distance decreased. Pointing especially suppressed descriptions of target location, suggesting that it was used to focus attention on a spatial region.},
author = {Bangerter, Adrian},
doi = {10.1111/j.0956-7976.2004.00694.x},
file = {:home/josiah/Documents/Mendeley Desktop/Bangerter - 2004 - Using pointing and describing to achieve joint focus of attention in dialogue.pdf:pdf},
isbn = {0956-7976},
issn = {09567976},
journal = {Psychological Science},
number = {6},
pages = {415--419},
pmid = {15147496},
title = {{Using pointing and describing to achieve joint focus of attention in dialogue}},
volume = {15},
year = {2004}
}
@article{Ravizza2003,
abstract = {The production of meaningful gestures has been claimed to enhance lexical access. However, the possibility that meaningless movements also improve retrieval has been largely ignored despite evidence that all types of movements increase with dysfluency. To examine this issue, we conducted two experiments to determine whether movements in general would improve lexical access in a tip-of-the-tongue (TOT) paradigm. TOT states were induced by presenting definitions of rare words that participants were then asked to recall. Participants who were required to tap at their own pace while retrieving words obtained significantly higher resolution rates than those who were immobile. Thus, movement does not have to be semantically related to the lexical item in order to aid in retrieval. However, tapping did not improve lexical access in all retrieval tasks. In a lexical retrieval task that relied more on executive abilities (letter fluency), participants who tapped retrieved fewer words than those who were immobile. The fact that movement enhanced lexical access only when retrieval depended on the automatic spread of activation suggests that facilitation may occur because of the activation of neural areas common to both speech and movement.},
author = {Ravizza, Susan},
doi = {10.3758/BF03196522},
file = {:home/josiah/Documents/Mendeley Desktop/Ravizza - 2003 - Movement and lexical access do noniconic gestures aid in retrieval.pdf:pdf},
issn = {1069-9384},
journal = {Psychonomic bulletin {\&} review},
number = {3},
pages = {610--615},
pmid = {14620354},
title = {{Movement and lexical access: do noniconic gestures aid in retrieval?}},
volume = {10},
year = {2003}
}
@article{Bavelas2002,
author = {Bavelas, Janet and Kenwood, Christine and Johnson, Trudy and Phillips, Bruce},
journal = {Gesture},
number = {1},
pages = {1--17},
publisher = {John Benjamins Publishing Company},
title = {{An experimental study of when and how speakers use gestures to communicate}},
volume = {2},
year = {2002}
}
@article{Rauscher1996,
abstract = {Abstract—In a within-subjects design that varied whether speakers were allowed to gesture and the difficulty of lexical access, speakers were videotaped as they described animated action cartoons to a listener. When speakers were permitted to gesture, they gestured more often during phrases with spatial content than during phrases with other content. Speech with spatial content was less fluent when speakers could not gesture than when they couid gesture; speech with nonspatial content was nol affected by gesture condition. Preventing gesturing increased the relative frequency of nonjuncture filled pauses in speech with spatial content, but not in speech with other con- tent. Overall, the effects of preventing speakers from gesturing resembled those of increasing the difficulty of lexical access by other means, except that the effects of gesture restriction were specific to speech with spatial content. The findings support the hypothesis that gestural accompaniments to spontaneous speech can facilitate access to the mental lexicon.},
author = {Rauscher, Frances H and Krauss, Robert M and Chen, Yihsiu},
doi = {10.1111/j.1467-9280.1996.tb00364.x},
file = {:home/josiah/Documents/Mendeley Desktop/Rauscher, Krauss, Chen - 1996 - Gesture, speech, and lexical access The role of lexical movements in speech production.pdf:pdf},
isbn = {0956-7976$\backslash$r1467-9280},
issn = {0956-7976},
journal = {Psychological Science},
number = {4},
pages = {226},
title = {{Gesture, speech, and lexical access: The role of lexical movements in speech production}},
url = {http://pss.sagepub.com/content/7/4/226.short},
volume = {7},
year = {1996}
}
@article{Melinger2007,
abstract = {In the present study we used a picture description task to investigate one contributing factor to gesture rate variation within speakers, namely conceptualisation load. Specifically we investigated whether speakers produce more gestures when the load on conceptualisation processes is higher. We manipulated conceptualisation load, without substantially changing speech formulation, either by modulating the complexity of the to-be-described pictures (Experiment 1) or by introducing a secondary task that uses the same or different resources as the primary description task (Experiment 2). In both studies, we find evidence that speakers produce more gestures at moments of relatively high conceptual load. The results indicate that the linearisation and the focusing components of conceptualisation are tied to gesture production and that increased load on these components results in increased gesture production.},
author = {Melinger, Alissa and Kita, Sotaro},
doi = {10.1080/01690960600696916},
file = {:home/josiah/Documents/Mendeley Desktop/Melinger, Kita - 2007 - Conceptualisation load triggers gesture production.pdf:pdf},
isbn = {0169-0965},
issn = {0169-0965},
journal = {Language and Cognitive Processes},
number = {4},
pages = {473--500},
title = {{Conceptualisation load triggers gesture production}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01690960600696916},
volume = {22},
year = {2007}
}
@article{Gerwing2011,
abstract = {{\textless}p{\textgreater} One measure of the communicative function of gestures is to test how speakers' gestures are influenced by whether an addressee can see them or not, that is, by manipulating visibility between participants. We question traditional dependent variables (i.e., rate measures), suggesting that they may have been insufficient for capturing essential differences in the gestures speakers use in each condition. We propose that investigating the qualitative features of gestures is a more nuanced, and ultimately more informative approach. We examined how speakers distributed information between their gestures and words, testing whether this distribution was affected by the visibility of their addressee. Twenty pairs of undergraduates took part in conversations that were either face to face ( {\textless}italic{\textgreater}n{\textless}/italic{\textgreater}  = 10) or on the telephone ( {\textless}italic{\textgreater}n{\textless}/italic{\textgreater}  = 10). Each speaker described a drawing of an elaborate dress to the addressee. We used a semantic feature analysis to analyze descriptions of the dress' skirt and assessed when words or gestures contributed information about five categories pertaining to features of the skirt's unusual shape. Although speakers' rates of gesturing and number of words did not vary significantly between conditions, speakers contributed more information and conveyed more categories in their gestures when the addressee would see them, while words carried the informational burden when addressees would not see the gestures ( {\textless}italic{\textgreater}p{\textless}/italic{\textgreater} 's {\textless} .001). These results suggest that gestures serve a communicative function. The semantic feature analysis is thus an example of how to explore gestures' qualitative features within a quantitative paradigm. {\textless}/p{\textgreater}},
author = {Gerwing, Jennifer and Allison, Meredith},
doi = {10.1075/gest.11.3.03ger},
file = {:home/josiah/Documents/Mendeley Desktop/Gerwing, Allison - 2011 - The flexible semantic integration of gestures and words Comparing face-to-face and telephone dialogues.pdf:pdf},
isbn = {1568-1475},
issn = {1568-1475},
journal = {Gesture},
number = {3},
pages = {308--329},
title = {{The flexible semantic integration of gestures and words: Comparing face-to-face and telephone dialogues}},
url = {http://www.jbe-platform.com/content/journals/10.1075/gest.11.3.03ger},
volume = {11},
year = {2011}
}
@article{Bavelas2008,
abstract = {Speakers often gesture in telephone conversations, even though they are not visible to their addressees. To test whether this effect is due to being in a dialogue, we separated visibility and dialogue with three conditions: face-to-face dialogue (10 dyads), telephone dialogue (10 dyads), and monologue to a tape recorder (10 individuals). For the rate of gesturing, both dialogue and visibility had significant, independent effects, with the telephone condition consistently higher than the tape recorder. Also, as predicted, visibility alone significantly affected how speakers gestured: face-to-face speakers were more likely to make life-size gestures, to put information in their gestures that was not in their words, to make verbal reference to their gestures, and to use more gestures referring to the interaction itself. We speculate that demonstration, as a modality, may underlie these findings and may be intimately tied to dialogue while being suppressed in monologue. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Bavelas, Janet and Gerwing, Jennifer and Sutton, Chantelle and Prevost, Danielle},
doi = {10.1016/j.jml.2007.02.004},
file = {:home/josiah/Documents/Mendeley Desktop/Bavelas et al. - 2008 - Gesturing on the telephone Independent effects of dialogue and visibility.pdf:pdf},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {Demonstration,Face-to-face dialogue,Gestures,Telephone,Visibility},
number = {2},
pages = {495--520},
title = {{Gesturing on the telephone: Independent effects of dialogue and visibility}},
volume = {58},
year = {2008}
}
@article{Pickering2006,
abstract = {Pickering and Garrod (2004) argued that alignment is the basis of successful communication in dialogue. In other words, successful communication goes hand-in-hand with the development of similar representations in the interlocutors. But what exactly does this mean? In this paper, we attempt to define alignment, contrasting alignment of situa- tion models with alignment of linguistic representations. We then speculate on how these notions are related and why they lead to conversational success.},
author = {Pickering, Martin J. and Garrod, Simon},
doi = {10.1007/s11168-006-9004-0},
file = {:home/josiah/Documents/Mendeley Desktop/Pickering, Garrod - 2006 - Alignment as the basis for successful communication.pdf:pdf},
isbn = {1116800690040},
issn = {15707075},
journal = {Research on Language and Computation},
keywords = {Alignment,Dialogue,Situation model},
number = {2-3},
pages = {203--228},
pmid = {16876778},
title = {{Alignment as the basis for successful communication}},
volume = {4},
year = {2006}
}
@article{Habets2011,
abstract = {During face-to-face communication, one does not only hear speech but also see a speaker's communicative hand movements. It has been shown that such hand gestures play an important role in communication where the two modalities influence each other's interpretation. A gesture typically temporally overlaps with coexpressive speech, but the gesture is often initiated before (but not after) the coexpressive speech. The present ERP study investigated what degree of asynchrony in the speech and gesture onsets are optimal for semantic integration of the concurrent gesture and speech. Videos of a person gesturing were combined with speech segments that were either semantically congruent or incongruent with the gesture. Although gesture and speech always overlapped in time, gesture and speech were presented with three different degrees of asynchrony. In the SOA 0 condition, the gesture onset and the speech onset were simultaneous. In the SOA 160 and 360 conditions, speech was delayed by 160 and 360 msec, respectively. ERPs time locked to speech onset showed a significant difference between semantically congruent versus incongruent gesture-speech combinations on the N400 for the SOA 0 and 160 conditions. No significant difference was found for the SOA 360 condition. These results imply that speech and gesture are integrated most efficiently when the differences in onsets do not exceed a certain time span because of the fact that iconic gestures need speech to be disambiguated in a way relevant to the speech context.},
author = {Habets, Boukje and Kita, Sotaro and Shao, Zeshu and Ozy{\"{u}}rek, Asli and Hagoort, Peter},
doi = {10.1162/jocn.2010.21462},
file = {:home/josiah/Documents/Mendeley Desktop/Habets et al. - 2011 - The role of synchrony and ambiguity in speech-gesture integration during comprehension.pdf:pdf},
isbn = {0898-929X},
issn = {1530-8898},
journal = {Journal of cognitive neuroscience},
keywords = {Adolescent,Analysis of Variance,Brain Mapping,Comprehension,Comprehension: physiology,Electroencephalography,Electroencephalography Phase Synchronization,Electroencephalography Phase Synchronization: phys,Electroencephalography: methods,Female,Gestures,Humans,Male,Photic Stimulation,Reaction Time,Speech,Speech: physiology,Young Adult},
number = {8},
pages = {1845--54},
pmid = {20201632},
title = {{The role of synchrony and ambiguity in speech-gesture integration during comprehension.}},
url = {http://www.mitpressjournals.org.proxy3.library.mcgill.ca/doi/abs/10.1162/jocn.2010.21462{\#}.VyQTr4QrK00},
volume = {23},
year = {2011}
}
@article{Cohen1973,
author = {Cohen, Akiba A and Harrison, Randall P},
journal = {Journal of Personality and Social Psychology},
number = {2},
pages = {276},
publisher = {American Psychological Association},
title = {{Intentionality in the use of hand illustrators in face-to-face communication situations.}},
volume = {28},
year = {1973}
}
@article{Bernardis2006,
abstract = {Humans speak and produce symbolic gestures. Do these two forms of communication interact, and how? First, we tested whether the two communication signals influenced each other when emitted simultaneously. Participants either pronounced words, or executed symbolic gestures, or emitted the two communication signals simultaneously. Relative to the unimodal conditions, multimodal voice spectra were enhanced by gestures, whereas multimodal gesture parameters were reduced by words. In other words, gesture reinforced word, whereas word inhibited gesture. In contrast, aimless arm movements and pseudo-words had no comparable effects. Next, we tested whether observing word pronunciation during gesture execution affected verbal responses in the same way as emitting the two signals. Participants responded verbally to either spoken words, or to gestures, or to the simultaneous presentation of the two signals. We observed the same reinforcement in the voice spectra as during simultaneous emission. These results suggest that spoken word and symbolic gesture are coded as single signal by a unique communication system. This signal represents the intention to engage a closer interaction with a hypothetical interlocutor and it may have a meaning different from when word and gesture are encoded singly. ?? 2005 Elsevier Ltd. All rights reserved.},
author = {Bernardis, Paolo and Gentilucci, Maurizio},
doi = {10.1016/j.neuropsychologia.2005.05.007},
file = {:home/josiah/Documents/Mendeley Desktop//Bernardis, Gentilucci - 2006 - Speech and gesture share the same communication system.pdf:pdf},
isbn = {0028-3932},
issn = {00283932},
journal = {Neuropsychologia},
keywords = {Arm kinematics,Social intention,Speech,Symbolic gesture,Voice spectra},
number = {2},
pages = {178--190},
pmid = {16005477},
title = {{Speech and gesture share the same communication system}},
volume = {44},
year = {2006}
}
@article{Gerwing2004,
abstract = {Hand gestures in face-to-face dialogue are symbolic acts, integrated with speech. Little is known about the factors that determine the physical form of these gestures. When the gesture depicts a previous nonsymbolic action, it obviously resembles this action; however, such gestures are not only noticeably different from the original action but, when they occur in a series, are different from each other. This paper presents an experiment with two separate analyses (one quantitative, one qualitative) testing the hypothesis that the immediate communicative function is a determinant of the symbolic form of the gesture. First, we manipulated whether the speaker was describing the previous action to an addressee who had done the same actions and therefore shared common ground or to one who had done different actions and therefore did not share common ground. The common ground gestures were judged to be significantly less complex, precise, or informative than the latter, a finding similar to the effects of common ground on words. In the qualitative analysis, we used the given versus new principle to analyze a series of gestures about the same actions by the same speaker. The speaker emphasized the new information in each gesture by making it larger, clearer, etc. When this information became given, a gesture for the same action became smaller or less precise, which is similar to findings for given versus new information in words. Thus the immediate communicative function (e.g., to convey information that is common ground or that is new) played a major role in determining the physical form of the gestures.},
author = {Gerwing, Jennifer and Bavelas, Janet},
doi = {10.1075/gest.4.2.04ger},
file = {:home/josiah/Documents/Mendeley Desktop/Gerwing, Bavelas - 2005 - Linguistic influences on gesture's form.pdf:pdf},
issn = {1568-1475},
journal = {Gesture},
number = {2},
pages = {157--195},
title = {{Linguistic influences on gesture's form}},
url = {http://www.jbe-platform.com/content/journals/10.1075/gest.4.2.04ger},
volume = {4},
year = {2005}
}
@article{Krauss1999,
abstract = {[Our account has relied primarily on data from experiments. Experimentation is, of course, a powerful method for generating certain kinds of data, but it also has serious limitations, and Kendon (1994) has remarked on discrepancies between the conclusions reached by investigators who rely on experimental findings and those whose data derive mainly from natural observation. Observational studies have enhanced our understanding of what gestures accomplish, and the recent addition of neuropsychological observations should provide further insight into the gesture production system. The conclusions of careful and seasoned observers certainly deserve to be taken seriously. At the same time, we are uncomfortable with some investigators' excessive reliance upon observers' impressions of gestural form or meaning, especially when the observers are aware of the contents of the accompanying speech. Without the proper controls, such impressions provide a weak foundation for theory and, we believe, are more usefully thought of as a source of hypotheses than as data in their own right. We have few illusions that we have considered every possible implementation of the model, or that all of the assumptions we have made will ultimately prove to have been justified. Indeed, our own ideas about the process by which lexical gestures are generated have changed considerably over the last several years, and we would be surprised if they did not continue to change as data accumulate. Although much of the data currently available is equivocal, and much more remains to be collected, we believe that the process in which models are produced and data (both experimental and observational) are collected to confirm or disconfirm them will ultimately result in a genuine understanding of how gestures are produced and how they are related to speech.]},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Krauss, Robert M and Hadar, Uri},
doi = {10.1080/14417040600667293},
eprint = {0402594v3},
file = {:home/josiah/Documents/Mendeley Desktop/Krauss, Hadar - 1999 - The Role of Speech-Related ArmHand Gestures in Word Retrieval.pdf:pdf},
isbn = {0-19-852451-X},
issn = {1754-9507},
journal = {Gesture, speech, and sign},
keywords = {gestures, hesitation, priming, lexical retrieval},
pages = {93--116},
pmid = {7096619},
primaryClass = {arXiv:cond-mat},
title = {{The Role of Speech-Related Arm/Hand Gestures in Word Retrieval}},
url = {http://www.google.com/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=1{\&}ved=0CFkQFjAA{\&}url=http://www.columbia.edu/{~}rmk7/PDF/K{\%}26H.pdf{\&}ei=157HT-fMM4TPhAfU5qC0Cw{\&}usg=AFQjCNHZzh3m5L3{\_}4EQ1Na5ce2FZ9gWkwQ{\&}sig2=5-DHpjgqUxXJvfY5Bo2F2w},
year = {1999}
}
@article{Pickering2004a,
abstract = {Traditional mechanistic accounts of language processing derive almost entirely from the study of monologue. Yet, the most natural and basic form of language use is dialogue. As a result, these accounts may only offer limited theories of the mechanisms that underlie language processing in general. We propose a mechanistic account of dialogue, the interactive alignment account, and use it to derive a number of predictions about basic language processes. The account assumes that, in dialogue, the linguistic representations employed by the interlocutors become aligned at many levels, as a result of a largely automatic process. This process greatly simplifies production and comprehension in dialogue. After considering the evidence for the interactive alignment model, we concentrate on three aspects of processing that follow from it. It makes use of a simple interactive inference mechanism, enables the development of local dialogue routines that greatly simplify language processing, and explains the origins of self-monitoring in production. We consider the need for a grammatical framework that is designed to deal with language in dialogue rather than monologue, and discuss a range of implications of the account.},
author = {Pickering, Martin J. and Garrod, Simon},
doi = {10.1017/S0140525X04000056},
file = {:home/josiah/Documents/Mendeley Desktop/Pickering, Garrod - 2004 - Toward a mechanistic psychology of dialogue.pdf:pdf},
isbn = {0140-525X (Print)$\backslash$r0140-525X (Linking)},
issn = {0140-525X},
journal = {The Behavioral and brain sciences},
keywords = {common ground,dialogue,dialogue routines,language comprehension,language production,monitoring,perception-be-},
number = {2},
pages = {169--190; discussion 190--226},
pmid = {15595235},
title = {{Toward a mechanistic psychology of dialogue.}},
volume = {27},
year = {2004}
}
@article{Mol2011,
abstract = {Speakers are argued to adapt their language production to their addressee's needs. For instance, speakers produce fewer and smaller hand gestures when interlocutors cannot see each other. Yet is this because speakers know their addressee cannot see them, or because they themselves do not see their addressee? By means of computer-mediated communication we manipulated these factors independently. We found that speakers took into account what their addressee saw. They produced more and larger gestureswhen they knew the addressee could see them. Seeing the addressee increased gesture production only if speakers could readily interpret the addressee's eye gaze, which is not usually the case inmediated interaction. Adding this affordance resulted in gesturing being similar in mediated and unmediated communication.},
author = {Mol, Lisette and Krahmer, Emiel and Maes, Alfons and Swerts, Marc},
doi = {10.1111/j.1083-6101.2011.01558.x},
file = {:home/josiah/Documents/Mendeley Desktop/Mol et al. - 2011 - Seeing and being seen The effects on gesture production.pdf:pdf},
isbn = {1083-6101},
issn = {10836101},
journal = {Journal of Computer-Mediated Communication},
keywords = {Eye-catcher,Gaze,Gesture,Mediated communication,Perspective taking,Theory of mind},
number = {1},
pages = {77--100},
title = {{Seeing and being seen: The effects on gesture production}},
volume = {17},
year = {2011}
}
@article{DeRuiter2012,
abstract = {The tradeoff hypothesis in the speech-gesture relationship claims that (a) when gesturing gets harder, speakers will rely relatively more on speech, and (b) when speaking gets harder, speakers will rely relatively more on gestures. We tested the second part of this hypothesis in an experimental collaborative referring paradigm where pairs of participants (directors and matchers) identified targets to each other from an array visible to both of them. We manipulated two factors known to affect the difficulty of speaking to assess their effects on the gesture rate per 100 words. The first factor, codability, is the ease with which targets can be described. The second factor, repetition, is whether the targets are old or new (having been already described once or twice). We also manipulated a third factor, mutual visibility, because it is known to affect the rate and type of gesture produced. None of the manipulations systematically affected the gesture rate. Our data are thus mostly inconsistent with the tradeoff hypothesis. However, the gesture rate was sensitive to concurrent features of referring expressions, suggesting that gesture parallels aspects of speech. We argue that the redundancy between speech and gesture is communicatively motivated.},
author = {de Ruiter, Jan P. and Bangerter, Adrian and Dings, Paula},
doi = {10.1111/j.1756-8765.2012.01183.x},
file = {:home/josiah/Documents/Mendeley Desktop/de Ruiter, Bangerter, Dings - 2012 - The Interplay Between Gesture and Speech in the Production of Referring Expressions Investigating t.pdf:pdf},
isbn = {1756-8757},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {Gesture,Gesture-speech redundancy,Gesture-speech tradeoff,Iconic gestures,Pointing,Referring expressions,Speech production},
number = {2},
pages = {232--248},
pmid = {22389109},
title = {{The Interplay Between Gesture and Speech in the Production of Referring Expressions: Investigating the Tradeoff Hypothesis}},
volume = {4},
year = {2012}
}
@article{Mol2011a,
abstract = {Speakers are argued to adapt their language production to their addressee's needs. For instance, speakers produce fewer and smaller hand gestures when interlocutors cannot see each other. Yet is this because speakers know their addressee cannot see them, or because they themselves do not see their addressee? By means of computer-mediated communication we manipulated these factors independently. We found that speakers took into account what their addressee saw. They produced more and larger gestureswhen they knew the addressee could see them. Seeing the addressee increased gesture production only if speakers could readily interpret the addressee's eye gaze, which is not usually the case inmediated interaction. Adding this affordance resulted in gesturing being similar in mediated and unmediated communication.},
author = {Mol, Lisette and Krahmer, Emiel and Maes, Alfons and Swerts, Marc},
doi = {10.1111/j.1083-6101.2011.01558.x},
file = {:home/josiah/Documents/Mendeley Desktop/Mol et al. - 2011 - Seeing and being seen The effects on gesture production.pdf:pdf},
isbn = {1083-6101},
issn = {10836101},
journal = {Journal of Computer-Mediated Communication},
keywords = {Eye-catcher,Gaze,Gesture,Mediated communication,Perspective taking,Theory of mind},
number = {1},
pages = {77--100},
title = {{Seeing and being seen: The effects on gesture production}},
volume = {17},
year = {2011}
}
@article{Alibali2001,
abstract = {Do speakers gesture to benefit their listeners? This study examined whether speakers use gestures differently when those gestures have the potential to communicate and when they do not. Participants watched an animated cartoon and narrated the cartoon story to a listener in two parts: one part in normal face-to-face interaction and one part with visibility between speaker and listener blocked by a screen. The session was videotaped with a hidden camera. Gestures were identified and classified into two categories: representational gestures, which are gestures that depict semantic content related to speech by virtue of handshape, placement, or motion, and beat gestures, which are simple, rhythmic gestures that do not convey semantic content. Speakers produced representational gestures at a higher rate in the face-to-face condition; however, they continued to produce some representational gestures in the screen condition, when their listeners could not see the gestures. Speakers produced beat gestures at comparable rates under both conditions. The findings suggest that gestures serve both speaker-internal and communicative functions.},
author = {Alibali, Martha W and Heath, Dana C and Myers, Heather J},
doi = {10.1006/jmla.2000.2752},
file = {:home/josiah/Documents/Mendeley Desktop/Alibali, Heath, Myers - 2001 - Effects of Visibility between Speaker and Listener on Gesture Production Some Gestures Are Meant to Be Se.pdf:pdf},
isbn = {0749-596X},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {bene t their listeners,communication,communicative,context,do speakers gesture to,gesture,many studies have demonstrated,narrative,spontaneous hand gestures have,that speakers},
pages = {169--188},
title = {{Effects of Visibility between Speaker and Listener on Gesture Production: Some Gestures Are Meant to Be Seen,}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X00927529},
volume = {44},
year = {2001}
}
@article{Gonseth2013,
abstract = {This paper explores the possible encoding of distance information in vocal and manual pointing and its relationship with the linguistic structure of deictic words, as well as speech/gesture cooperation within the process of deixis. Two experiments required participants to point at and/or name a close or distant target, with speech only, with gesture only, or with speech + gesture. Acoustic, articulatory, and manual data were recorded. We investigated the interaction between vocal and manual pointing, with respect to the distance to the target. There are two major findings. First, distance significantly affects both articulatory and manual pointing, since participants perform larger vocal and manual gestures to designate a more distant target. Second, modality influences both deictic speech and gesture, since pointing is more emphatic in unimodal use of either over bimodal use of both, to compensate for the loss of the other mode. These findings suggest that distance is encoded in both vocal and manual pointing. We also demonstrate that the correlates of distance encoding in the vocal modality can be related to the typology of deictic words. Finally, our data suggest a two-way interaction between speech and gesture, and support the hypothesis that these two modalities are cooperating within a single communication system. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Gonseth, Chloe and Vilain, Anne and Vilain, Coriandre},
doi = {10.1016/j.specom.2012.11.003},
file = {:home/josiah/Documents/Mendeley Desktop/Gonseth, Vilain, Vilain - 2013 - An experimental study of speechgesture interactions and distance encoding.pdf:pdf},
isbn = {9789814401494},
issn = {01676393},
journal = {Speech Communication},
keywords = {Distance encoding,Pointing,Sound symbolism,Speech/gesture interaction},
number = {4},
pages = {553--571},
title = {{An experimental study of speech/gesture interactions and distance encoding}},
url = {http://dx.doi.org/10.1016/j.specom.2012.11.003},
volume = {55},
year = {2013}
}
@article{Morrel-Samuels1992,
abstract = {Seventeen Ss were videotaped as they provided narrative descriptions of 13 photographs. Judgments from 129 naive untrained Ss were used to isolate 60 speech-related gestures and their lexical affiliates (i.e., the accompanying word or phrase judged as related in meaning) from these 221 narratives. A computer-video interface measured each gesture, and a 3rd group of Ss rated word familiarity of each lexical affiliate. Multiple regression revealed that gesture onset preceded voice onset by an interval whose magnitude was inversely related to the lexical affiliate's rated familiarity. The lexical affiliate's familiarity was also inversely related to gesture duration. Results suggest that difficulty encountered during lexical access affects both gesture and speech. Familiarity's systematic relations with gesture-speech asynchrony and gesture duration make it unlikely that speech and gesture are produced independently by autonomous modules.},
author = {Morrel-Samuels, Palmer and Krauss, Robert M.},
doi = {10.1037/0278-7393.18.3.615},
file = {:home/josiah/Documents/Mendeley Desktop/Morrel-Samuels, Krauss - 1992 - Word Familiarity Predicts Temporal Asynchrony of Hand Gestures and Speech.pdf:pdf},
issn = {0278-7393},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
number = {3},
pages = {615--622},
title = {{Word Familiarity Predicts Temporal Asynchrony of Hand Gestures and Speech}},
volume = {18},
year = {1992}
}
@article{So2009,
abstract = {[In order to produce a coherent narrative, speakers must identify the characters in the tale so that listeners can figure out who is doing what to whom. This paper explores whether speakers use gesture, as well as speech, for this purpose. English speakers were shown vignettes of two stories and asked to retell the stories to an experimenter. Their speech and gestures were transcribed and coded for referent identification. A gesture was considered to identify a referent if it was produced in the same location as the previous gesture for that referent. We found that speakers frequently used gesture location to identify referents. Interestingly, however, they used gesture most often to identify referents that were also uniquely specified in speech. Lexical specificity in referential expressions in speech thus appears to go hand-in-hand with specification in referential expressions in gesture.]},
author = {So, Wing Chee and Kita, Sotaro and Goldin-Meadow, Susan},
doi = {10.1111/j.1551-6709.2008.01006.x},
file = {:home/josiah/Documents/Mendeley Desktop/So, Kita, Goldin-Meadow - 2009 - Using the hands to identify who does what to whom Gesture and speech go hand-in-hand.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Communication,Discourse,Gesture,Referential expression},
number = {1},
pages = {115--125},
pmid = {20126430},
title = {{Using the hands to identify who does what to whom: Gesture and speech go hand-in-hand}},
volume = {33},
year = {2009}
}
@article{Jacobs2007,
author = {Jacobs, Naomi and Garnham, Alan},
journal = {Journal of Memory and Language},
number = {2},
pages = {291--303},
publisher = {Elsevier},
title = {{The role of conversational hand gestures in a narrative task}},
volume = {56},
year = {2007}
}
@article{Krauss2000,
  title={Lexical gestures and lexical access: a process model},
  author={Krauss, Robert M and Chen, Yihsiu and Gotfexnum, Rebecca F},
  journal={Language and gesture},
  volume={2},
  pages={261},
  year={2000}
}

